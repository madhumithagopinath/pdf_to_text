{
  "Page_1": "UNIT IT INTRODUCTION TO DEEP LEARNING\n\n    \n\nDeep Feed Forward Neural Networks \u2014 Gradient Descent \u2014 Back Propagation and other Different{ation\nAlgorithms- Vanishing Gradient Problem- Mitigation \u2014 Rectified Linear Unit(ReLU)- Heuristics for Avgiding\nBad Local Minima \u2014 Heuristics for Faster Training \u2014 Nestors Accelerated Gradient Descent \u2014 Regulariqation\nfor Deep Learning \u2014 Dropout \u2014 Adversial Training \u2014 Optimization for Training Deep models.\n\n \n\nHistory of Deep Learning [DL]:\n\nQ) The chain rule that underlies the back-propagation algorithm was invented in the\nseventeenth century (Leibniz, 1676; L\u2019H6pital, 1696)\n\nQ) Beginning in the 1940s, the function approximation techniques were used to motivate\nmachine learning models such as the perceptron\n\nThe earliest models were based on linear models. Critics including Marvin Minsky\npointed out several of the flaws of the linear model family, such as its inability to learn\nthe XOR function, which led to a backlash against the entire neural network approach\nEfficient applications of the chain rule based on dynamic programming began to appear\nin the 1960s and 1970s\n\nWerbos (1981) proposed applying chain rule techniques for training artificial neural\nnetworks. The idea was finally developed in practice after being independently\nrediscovered in different ways (LeCun, 1985; Parker, 1985; Rumelhart et al., 1986a)\nFollowing the success of back-propagation, neural network research gained popularity\nand reached a peak in the early 1990s. Afterwards, other machine learning techniques\nbecame more popular until the modern deep learning renaissance that began in 2006\nThe core ideas behind modern feedforward networks have not changed substantially\nsince the 1980s. The same back-propagation algorithm and the same approaches to\ngradient descent are still in use.\n\nMost of the improvement in neural network performance from 1986 to 2015 can be\nattributed to two factors. First, larger datasets have reduced the degree to which statistical\ngeneralization is a challenge for neural networks. Second, neural networks have become much\nlarger, because of more powerful computers and better software infrastructure. A small\nnumber of algorithmic changes have also improved the performance of neural networks\nnoticeably. One of these algorithmic changes was the replacement of mean squared error with\nthe cross-entropy family of loss functions. Mean squared error was popular in the 1980s and\n1990s but was gradually replaced by cross-entropy losses and the principle of maximum\nlikelihood as ideas spread between the statistics community and the machine learning\ncommunity.\n\n \n\nThe other major algorithmic change that has greatly improved the performance of\nfeedforward networks was the replacement of sigmoid hidden units with piecewise linear\nhidden units, such as rectified linear units. Rectification using the max{0, z} function was\nintroduced in early neural network models and dates back at least as far as the Cognitron and\nNeo-Cognitron (Fukushima, 1975, 1980).\n\nFor small datasets, Jarrett et al. (2009) observed that using rectifying nonlinearities is\neven more important than learning the weights of the hidden layers. Random weights are\n\nsufficient to propagate useful information through a rectified linear network, enabling the\nclassifier layer at the top to learn how to map different feature vectors to class identities. When\nmore data is available, learning begins to extract enough useful knowledge to exceed the\n\n1",
  "Page_2": "performance of randomly chosen parameters. Glorot et al. (2011a) showed that learning is far\neasier in deep rectified linear networks than in deep networks that have curvature or two-sided\nsaturation in their activation functions.\n\nWhen the modern resurgence of deep learning began in 2006, feedforward networks\ncontinued to have a bad reputation. From about 2006 to 2012, it was widely believed that\nfeedforward networks would not perform well unless they were assisted by other models, such\nas probabilistic models. Today, it is now known that with the right resources and engineering\npractices, feedforward networks perform very well. Today, gradient-based learning in\nfeedforward networks is used as a tool to develop probabilistic models. Feedforward networks\ncontinue to have unfulfilled potential. In the future, we expect they will be applied to many\nmore tasks, and that advances in optimization algorithms and model design will improve their\nperformance even further.\n\nDeep Neural Network\n\nretraining)\nMulti-layered Ea { 2\n\nXOR Perceptron\nADALINE (Backpropagation)\n\nPerceptron\nGolden Age Dark Age (\u201cAl Winter\u201d)\nElectronic Brain\n\n1960 1970 1980 1990 rut) rut)\n\nA DGS ALP a 2P\n\nS.McCulloch-W. Pitts F.Rosenblatt \u2014B., Widrow ~ M. Hoff M, Minsky - S, Papert D, Rumelhart-G,Hinton-R. Wiliams __V. Vapnik ~C. Cortes G Hinton = 5 Ruslan\n\n \n\neKo @o] | >\n\npe\n\ne,@ 0}\n\n \n\n \n\n \n\n \n\n\u2014 Bectnad Err\n\n \n\n \n\n \n\n* Adjustable Weights + Leamable Weights and Threshold + XOR Problem + Solution ononlinearty separable problems + Limitations of arin rir knowiedge + Hirarhica fate Learing\n+ Weights are not Leamed + Big computation, loca optima ard overting + Kemelfuncbon: Human Intervention\n\n2.1 A Probabilistic Theory of Deep Learning\n\nProbability is the science of quantifying uncertain things. Most of machine learning and deep\nlearning systems utilize a lot of data to learn about patterns in the data. Whenever data is utilized\nin a system rather than sole logic, uncertainty grows up and whenever uncertainty grows up,\nprobability becomes relevant.\n\nBy introducing probability to a deep learning system, we introduce common sense to the\nsystem. In deep learning, several models like Bayesian models, probabilistic graphical models,\nHidden Markov models are _ used. They depend entirely on probability concepts.\n\nReal world data is chaotic. Since deep learning systems utilize real world data, they require a\ntool to handle the chaotic Ness.",
  "Page_3": "Deep Feedforward Networks\n\n* Deep feedforward networks, also often called feedforward neural networks, or\nmultilayer perceptrons (MLPs).\n\n* The goal of a feedforward network 1s to approximate some function / *.\n\n\u00a2 A feedforward network defines a mapping yv = f (x; 8) and learns the value of the\nparameters @ that result in the best function approximation.\n\n* These models are called feedforward because information flows through the\nfunction being evaluated from x, through the intermediate computations used to\ndefine f, and finally to the output y.\n\n* There are no feedback connections in which outputs of the model are fed back intq\nitself. When feedforward neural networks are extended to include feedback\nconnections, they are called recurrent neural Networks.\n\n\u00a2 Feedforward neural networks are called networks because they are\ntypically represented by composing together many different functions.\n\n\u00a2 The model is associated with a directed acyclic graph describing how the\nfunctions are composed together.\n\n\u00a2 For example, we might have three functions f(1) ,f(2), and f(3) connected\nin a chain, to form f(x) = f(3)(f (2) C)(x))).\n\n\u00a2 These chain structures are the most commonly used structures of neural\nnetworks.\n\n\u00a2 In this case, f(1) is called the first layer of the network, f(2) is called the\nsecond layer, and so on.\n\n\u00a2 The overall length of the chain gives the depth of the model.\n\n\u00a2 The final layer of a feedforward network is called the output layer. During\nneural network training, we drive f(x) to match f(x).\n\n\u00a2 The training data provides us with noisy, approximate examples of f *(x)\nevaluated at different training points.",
  "Page_4": "* Each example x is accompanied by a label y = f (x).\n\n* The training examples specify directly what the output layer must do at each point x; it must\nproduce a value that is close to y.\n\n* The behavior of the other layers is not directly specified by the training data.\n\n* The learning algorithm must decide how to use those layers to produce the desired output, but\nthe training data does not say what each individual layer should do.\n\n* Instead, the learning algorithm must decide how to use these layers to best implement an\napproximation of. tS\n\n* Because the training data does not show the desired output for each of these layers, these layer}\nare called hidden layers.\n\n* Finally, these networks are called nevral because they are loosely inspired by neuroscience.\n* Each hidden layer of the network is typically vector-valued.\n* The dimensionality of these hidden layers determines the width of the model.\n\n* Each unit resembles a neuron in the sense that it receives input from many other units and\ncomputes its own activation value.\n\n* The idea of using many layers of vector-valued representation is drawn from neuroscience.\n\n* The choice of the functions f(i)(x) used to compute these representations is also loosely guided\nby neuroscientific observations about the functions that biological neurons compute.\n\n* One way to understand feedforward networks 1s to begin with linear models and consider how to\novercome their limitations.\n\n* Linear models, such as logistic regression and linear regression, are appealing because they may be fit\nefficiently and reliably, either in closed form or with convex optimization.\n\n* Linear models also have the obvious defect that the model capacity is limited to linear functions, so the\nmodel cannot understand the interaction between any two input variables.\n\n* To extend linear models to represent nonlinear functions of x, we can apply the linear model not to x itself\nbut to a transformed input g(x), where g is a nonlinear transformation.\n\n* The question 1s then how to choose the mapping 9.\n\n1. One option is to use a very generic g, such as the infinite-dimensional g that 1s implicitly used by kemel\nmachines based on the RBF kernel.\n\n2. Another option is to manually engineer @.\n\n3. The strategy of deep learning is to learn g. In this approach, we have a model y = f(x; 1\u201d) = g(x; 6)\" w.\nWe now have parameters @ that we use to learn g from a broad class of functions, and parameters w that\nmap from g(x) to the desired output. This 1s an example of a deep feedforward network, with g defining a\nhidden layer. This approach is the only one of the three that gives up on the convexity of the traming\nproblem, but the benefits outweigh the harms. In this approach, we parametrize the representation as 9(x; 4)\nand use the optimization algorithm to find the @ that corresponds to a good representation.\n\n4",
  "Page_5": "Example: Learning XOR\n\n\u00a2 The XOR function (\u201cexclusive or\u201d) is an operation on two binary values, x1 and x2.\n\u00a2 When exactly one of these binary values is equal to 1, the XOR function returns 1.\n\n* Otherwise, it returns 0. The XOR function provides the target function y = f(x) that we\nwant to learn.\n\nEvaluated on our whole training set, the MSE loss function is\n\nJ(@) - S= (s* (@) \u2014 f(x: 0) . (6.1)\n\nrex\n\nNow we must choose the form of our model, f(x;@). Suppose that we choose\na linear model, with @ consisting of w and b. Our model is defined tp be\n\nf(a: w,b) = a2! w+ b. (6.2)\n\nWe can minimize J(@) in closed form with respect to w and 6 using the normal\nequations.\n\n\u00a2 After solving the normal equations, we obtain w = 0 and b = .\n\n\u00a2 . The linear model simply outputs 0.5 everywhere.",
  "Page_6": "Original a2 space Learned h space\n\n1\nAy\n\nFigure 6.1: Solving the XOR problem by learning a representation. The bold numbers\nprinted on the plot indicate the value that the learned function must output at each point.\n(Left)A linear model applied directly to the original input cannot implement the XOR\nfunction. When \u00ab2, = 0, the model\u2019s output must increase as 22 increases. When x7, = 1,\nthe model\u2019s output must decrease as r2 increases. A linear model must apply a fixed\ncoefficient wy to rg. The linear model therefore cannot use the value of +; to change\nthe coefficient on x2 and cannot solve this problem. (Rightj/In the transformed space\nrepresented by the features extracted by a neural network, a linear model can now solve\nthe problem. In our example solution, the two points that must have output 1 have been\ncollapsed into a single point in feature space. In other words, the nonlinear features have\nmapped both w = [1,0] 7 and 2 = [0,1]' to a single point in feature space, h = [1,0] \u00b0.\nThe linear model can now describe the function as increasing in /, and decreasing in hy.\nIn this example, the motivation for learning the feature space is only to make the model\ncapacity greater so that it can fit the training set. In more realistic applications, learned\nrepresentations can also help the model to generalize.\n\nBi\n\nFigure 6.2: An example of a feedforward network, drawn in two different styles. Specifically,\nthis is the feedforward network we use to solve the XOR example. It has a single hidden\nlayer containing two units. (Left)In this style, we draw every unit as a node in the graph,\nThis style is very explicit and unambiguous but for networks larger than this example\nit can consume too much space. (Right)In this style, we draw a node in the graph for\neach entire vector representing a layer\u2019s activations. This style is much more compact,\nSometimes we annotate the edges in this graph with the name of the parameters that\ndescribe the relationship between two layers. Here, we indicate that a matrix W describes\nthe mapping from a2 to h, and a vector w describes the mapping from h to y. We\ntypically omit the intercept parameters associated with each layer when labeling this kind\nof drawing.",
  "Page_7": "2}\n\nmax{0,\n\n)=\n\n9(2)\n\nFigure 6.3: The rectified linear activation function, This activation function is the default\nactivation function recommended for use with most feedforward neural networks. Applying\nthis function to the output of a linear transformation yields a nonlinear transformation.\nHowever, the function remains very close to linear, in the sense that is a piecewise linear\nfunction with two linear pieces. Because rectified linear units are nearly linear, they\npreserve many of the properties that make linear models easy to optimize with gradient-\nbased methods. They also preserve many of the properties that make linear models\ngeneralize well. A common principle throughout computer science is that we can build\ncomplicated systems from minimal components. Much as a Turing machine\u2019s memory\nneeds only to be able to store 0 or 1 states, we can build a universal function approximator\nfrom rectified linear functions.\n\nGradient Descent\n\n1. Definition:\n\n\u00a2 Anoptimization algorithm used to minimize a cost (loss) function by iteratively updating\n\nmodel parameters.\n2. Steps:\n\u00a2 Start with initial parameters (wo, bo).\n\n\u00a2 Update parameters using the gradient of the cost function:\n\nOJ OJ\nTe,\u2019 best = by - \"9b,\"\n\nwhere 1 is the learning rate, and J is the cost function.\n\nWii = We \u2014",
  "Page_8": "3. Types:\n\ne Batch Gradient Descent:\n\ne Uses the entire dataset for each update.\n\ne Slow for large datasets.\n\ne Stochastic Gradient Descent (SGD):\n\ne Updates parameters after each training sample.\ne Faster but introduces noise.\n\ne Mini-Batch Gradient Descent:\n\ne Acompromise between batch and stochastic approaches.",
  "Page_9": "2.2 Back Propagation Networks (BPN)\n2.2.1. Need for Multilayer Networks\n\ne Single Layer networks cannot used to solve Linear Inseparable problems &\ncan only be used to solve linear separable problems\nSingle layer networks cannot solve complex problems\nSingle layer networks cannot be used when large input-output data set is\navailable\nSingle layer networks cannot capture the complex information\u2019s available in\nthe training pairs\n\nHence to overcome the above said Limitations we use Multi-Layer Networks.\n\n2.2.2. Multi-Layer Networks\n\ne Any neural network which has at least one layer in between input and output\nlayers is called Multi-Layer Networks\n\ne Layers present in between the input and out layers are called Hidden Layers\n\neInput layer neural unit just collects the inputs and forwards them to the next\nhigher layer\n\ne Hidden layer and output layer neural units process the information\u2019s feed to\nthem and produce an appropriate output\n\ne Multi -layer networks provide optimal solution for arbitrary classification\nproblems\n\ne Multi -layer networks use linear discriminants, where the inputs are non\nlinear\n\n2.2.3. Back Propagation Networks (BPN)\n\nIntroduced by Rumelhart, Hinton, & Williams in 1986. BPN is a Multi-\nlayer Feedforward Network but error is back propagated, Hence the name Back\nPropagation Network (BPN). It uses Supervised Training process; it has a\nsystematic procedure for training the network and is used in Error Detection and\nCorrection. Generalized Delta Law /Continuous Perceptron Law/ Gradient Descent\nLaw is used in this network. Generalized Delta rule minimizes the mean squared\nerror of the output calculated from the output. Delta law has faster convergence rate\nwhen compared with Perceptron Law. It is the extended version of Perceptron\nTraining Law. Limitations of this law is the Local minima problem. Due to this the\nconvergence speed reduces, but it is better than perceptron\u2019s. Figure 1 represents a\nBPN network architecture. Even though Multi level perceptron\u2019s can be used they\nare flexible and efficient that BPN. In figure 1 the weights between input and the\nhidden portion is considered as Wij and the weight between first hidden to the next\nlayer is considered as Vjx. This network is valid only for Differential Output\nfunctions. The Training process used in backpropagation involves three stages,\nwhich are listed as below\n\n1. Feedforward of input training pair",
  "Page_10": "2. Calculation and backpropagation of associated error\n\n3. Adjustments of weights\n\n \n\nWi Vik\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nx C) vii C) Output layer\n\u00a5\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\na a\nOSGO\na =\n\u00aba\nOHO\n\nInput layer Hidden layer 1 Hidden layer 2\n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\nFigure 1: Back Propagation Network\n2.2.4. BPN Algorithm\nThe algorithm for BPN is as classified int four major steps as follows:\nInitialization of Bias, Weights\n\nFeedforward process\n\n1\n2\n3. Back Propagation of Errors\n4\n\nUpdating of weights & biases\nAlgorithm:\n\nI. Initialization of weights:\nStep 1: Initialize the weights to small random values near zero\nStep 2: While stop condition is false , Do steps 3 to 10\nStep 3: For each training pair do steps 4 to 9\nII. Feed forward of inputs\nStep 4: Each input xi is received and forwarded to higher layers (next\nhidden)\nStep 5: Hidden unit sums its weighted inputs as follows\nZinj = Woj + XUxiwij\nApplying Activation function\nZj = f(Zinj)\nThis value is passed to the output layer\nStep 6: Output unit sums it\u2019s weighted inputs\nYink= Voj + X ZjVjx\nApplying Activation function\n\n10",
  "Page_11": "Yk = f(yink)\nIll. Backpropagation of Errors\n\nStep 7: dk = (tk \u2014\nY)f(yink ) Step 8:\nSinj = X OjVix\nIV. Updating of Weights & Biases\nStep 8: Weight correction is\nA\nWij = 06xZ; bias Correction\nis\nA\nWoj = 06k\nV. Updating of Weights & Biases\nStep 9: continued:\nNew Weight is\nWij(new) = Wijold) +\nAwij Vik(new) =\nViola + AVix\nNew bias is\nWoj(new) = Woj(old)\n+ AWoj Vok(new) =\nVok(old) + AVok\n\nStep 10: Test for Stop Condition\n\n2.2.5 Merits\n* Has smooth effect on weight correction\n* Computing time is less if weight\u2019s are small\n* 100 times faster than perceptron model\n* Has a systematic weight updating procedure\n\n2.2.6. Demerits\nLearning phase requires intensive calculations\nSelection of number of Hidden layer neurons is an issue\nSelection of number of Hidden layers is also an issue\nNetwork gets trapped in Local Minima\nTemporal Instability\nNetwork Paralysis\nTraining time is more for Complex problems",
  "Page_12": "Assume that the neurons have a sigmoid activation function,\n\nperform a forward pass and a backward pass on the network.\n\nAssume that the actual output of y is 0.5 and learning rate is 1.\n\nPerform another forward pass.\n\nW,,=0. 6\nForward Pass: Compute output for y3, y4 and y5.\n\na= Oms*x) yi = Fai) =\n\nJ\nal= (wi3 *x1) + (w23 *x2)\n\n= (0.1 * 0.35) + (0.8 * 0.9) = 0.755\n\n\u2014 \u2014 1 =\ny3=Hla-ay ppp 7 0.688\n\na2= (wi4 *x1) + (w24* x2)\n\n= (0.4 * 0.35) + (0.6 * 0.9) = 0.68\n\n= f(ar 2)= aa = 0.6637\n\na3 = (w3s *y3) + (was * ya)",
  "Page_13": "= (0.3 * 0.68) + (0.9 * 0.6637) = 0.801\n\n1\n\ny5 = fas) - (1+e-0.801)\n\n= 0.69 Network Output\n\nError=y farget\u2014 ys = 0.5-0.69= 0.19\n\nAw, =70,0,\n\n6,=0,(l-o,)(\u00a2,-0,) if isan output unit\n6, =0,(1-o, >> OW, if j isa hidden unit\nk\n\nwhere n is a constant called the learning rate\n\nty is the correct teacher output for unit j\n\nPe\n\nQj is the error measure for unit j\n\nBackward Pass: Compute 63, 54 and 45.\n\nFor output unit:\n65 = y(1-y) (Ytarget \u2014 Y).\n= 0.69*(1-0.69)*(0.5-0.69)= -0.0406\n\nFor hidden unit:\n\n53=Y3(1-Y3) W35 * 55\n= 0.68*(1 -0.68)*(0.3* -0.0406) = -0.00265\n\n54=Y4(1-Y4)W45* 55\n=0.6637 * (1-0.6637)* (0.9 * -0.0406) =-0.0082",
  "Page_14": "Compute new weights\n\nAw]j = 1 901\n\nAW45= 1 85 Y4= 1* -0.0406 * 0.6637 = -0.0269\n\nW45 (new) = AW45 + W45 (old) = -0.0269 + (0.9) = 0.8731\n\nAw14=184x1 = 1* -0.0082 * 0.35 = -0.00287\nW14 (new) = AW14 + W14 (old) = -0.00287+ 0.4 = 0.3971\n\n\u00a2 Similarly, update all other weights\n\nUpdated w,,\n\nUl\n0.1 -0.00265 0.35 1\n0. -0.00265 0.9 1\n0.4 -0.0082 0.35 1\n1\n1\n1\n\n0.6 -0.0082 0.9\n\n0.3 -0.0406 0.68\n\n0.9 -0.0406 0.6637\ny; = 0.68\n\nn= os @)~w,-0 221\u201d\n: x . >\nW,, =0.3971 40\nW,; =0.7976\u201c a\nya \u2014 H, )W,;=0.8731 Ontpaty\nW,,=0.5926 \\_*/\ny, = 0.6637\n\nAgain need to apply the feed forward",
  "Page_15": "hain rule of differentiation in backpropagation\n\nC\nNeural Network\n\nHidden layer:\n\nH11=0.68, O(h11) =0.66 O1(target output)=0 ,y1=0.1636, O(y1) =0.54\nH12=0.76, O(h12)=0.68 O2(target output) =1, y2=0.324, O(y2)=0.58\nH21=1.306, O(h21) =0.786\n\nH22=1.504, O(h22) =0.818\nWhew = Wold- learning rate * slope\nWhew = Wold- * 0L(w)/ ow\n\n- The unknown is derivative ie, slope\n\nLoss=0.234, Learning rate =0.1\n\n- OL/ 6wi=0L/ 6 O(y2). 6 O(y2)/ dy2. dy2/\u00e9wr\n\n=-0.42 * 0.58*0.42*0.818 = -0.0836\n\nOL / 6 O(y2) = A/ 6 O(y2) ./21( O(y1)-01)*+( O(y2)-02)\"I\n-- Value of L\n\n= 2.1/2 ( O (y2)-O2) -- differentiate w.r.t y2\n\n= O(y2)-Oo\n= 0.58-1 =-0.42",
  "Page_16": "6 O(y2)/ dy2= 6 O(x)/ Ox =d/ Ox(1/1+e*)\n= [ formula f\u2019(x) = f(x)(1-f(x))]\n= (1/1+e*) (1-1/1+e*)\n= (x). (1- O@))\nTherefore, 0 O(y2)/ dy2 = O(y2). (1- O(y2))\n= 0.58 (1-0.58)\n=0.58*0.42\nOy2/Owi2= 6/Owi2(w12* O(h22)+w10 O(h21)+b2)\n\n= O(h22) =0.818 (it is partial derivative so other are nullify)\n\nWin =Wl2- aL / \u00e9wi2\n= 0.3 -0.1*-0.0836 => 0.30836\n\nThe vanishing gradient problem\n\nIt is a challenge faced in training artificial neural networks, particularly\ndeep feedforward and recurrent neural networks. This issue arises\nduring the backpropagation process, which is used to update the\nweights of the neural network through gradient descent. The gradients\nare calculated using the chain rule and propagated back through the\nnetwork, starting from the output layer and moving towards the input\nlayer. However, when the gradients are very small, they can diminish\nas they are propagated back through the network, leading to minimal\nor no updates to the weights in the initial layers. This phenomenon is\nknown as the vanishing gradient problem.",
  "Page_17": "ACG OSA) oe\n\nPX\nOy)\n\nae\n\nWR)\n\nSASS\naw XO RS \u201csh\nfae EN\nwr on Sa Sed OS oi a\nPM DN\nCRI PORK\n4 Es UAE YOR\nne K BO HS VY LX ry\na es oN BE\n\nTSX] a\nEe\n\niy,\nmid\n\nCertain activation functions, like the sigmoid function, squishes a large input space into a\nsmall input space between 0 and 1. Therefore, a large change in the input of the sigmoid\n\nfunction will cause a small change in the output. Hence, the derivative becomes small.\n\nAs an example, Image 1 is the sigmoid function and its derivative. Note how\nwhen the inputs of the sigmoid function becomes larger or smaller (when |x|\n\nbecomes bigger), the derivative becomes close to zero.",
  "Page_18": "3. However, when n hidden layers use an activation like the sigmoid\n\nfunction n small derivatives are multiplied together. Thus, the gradient\ndecreases exponentially as we propagate down to the initial layers.\n\n. Asmall gradient means that the weights and biases of the initial layers will\nnot be updates effectively with each training session, since the initial layers\nare often crucial to recognizing the core elements of the input data, it can\nlead to overall inaccuracy of the whole network.\n\nWhy it\u2019s Significant:\n\n1. For shallow network with only a few layers that use these activations, this\nisn\u2019t a big problem. However, when more layers are used, it can cause the\ngradient to be too small for training to work effectively.\n\n. Gradients of neural network are found using backpropagation, simply put\nbackpropagation finds the derivatives of the network by moving layer from\nthe final layer to the initial one. By the chain rule the derivatives of each\nlayer are multiplied down the network (from the final layer to the initial)\n\nto compute the derivatives of the initial layers.",
  "Page_19": "To mitigate the vanishing gradient problem, several strategies\nhave been developed:\n\nLong Short-Term Memory(LSTM)\n\n@ Soas of now, we have seen there are two major factors that affect the gradient size - weights\nand their derivatives of the activation function. A simple LSTM helps the gradient size to\nremain constant, The activation function we use in the LSTM often works as an identity\nfunction which is a derivative of 1. So in gradient backpropagation, the size of the gradient\ndoes not vanish,\n\nResidual Neural Network\n\n@ The skip or bypass connection inr residual network is useful in any network to bypass the\ndata from a few layers, Basically, it allows information to skip the layers. Using these\nconnections, information can be transferred from layer n to layer n+t. Here to perform this\nthing we need to connect the activation function of layer n to the activation function of\nn+t, This causes the gradient to pass between the layers without any modification in size,\n\nReLu Activation Function\n\ne Ifthe ReLU function is used for activation in a neural network in place of a sigmoid\nfunction, the value of the partial derivative of the loss function will be having values\nof 0 or 1 which prevents the gradient from vanishing. The use of ReLU function thus\nprevents the gradient from vanishing\n\nActivation Functions: Using activation functions such as Rectified Linear\nUnit (ReLU) and its variants (Leaky ReLU, Parametric ReLU, etc.) can help prevent\nthe vanishing gradient problem. ReLU and its variants have a constant gradient for\npositive input values, which ensures that the gradients do not diminish too quickly\nduring backpropagation.\n\nWeight Initialization: Properly initializing the weights can help prevent gradients\nfrom vanishing.",
  "Page_20": "Activation Function\n\nAn activation function in a neural network defines how the weighted sum of the input is\ntransformed into an output from a node or nodes in a layer of the network.\n\nSometimes the activation function is called a \u201ctransfer function.\u201d If the output range of the\nactivation function is limited, then it may be called a \u201csquashing function.\u201d Many activation\nfunctions are nonlinear and may be referred to*as the \u201cnonlinearity\u201d in the layer or the network\ndesign.\n\nThe choice of activation function has a large impact on the capability and performance of the\nneural network, and different activation functions may be used in different parts of the model.\nTechnically, the activation function is used within or after the internal processing of each node in\nthe network, although networks are designed to use the same activation function for all nodes in\na layer.\n\nAnetwork may have three types of layers: input layers that take raw input from the domain,\nhidden layers that take input from another layer and pass output to another layer, and output\nlayers that make a prediction.\n\nAll hidden layers typically use the same activation function. The output layer will typically use a\ndifferent activation function from the hidden layers and is dependent upon the type of prediction\nrequired by the model.\n\nActivation functions are also typically differentiable, meaning the first-order derivative can be\ncalculated for a given input value. This is required given that neural networks are typically trained\nusing the backpropagation of error algorithm that requires the derivative of prediction error in\norder to update the weights of the model.\n\nActivation Function Choices for Hidden Layers\n\n=~",
  "Page_21": "BINARY STEP FUNCTION\n\nPex = 34 2\n1\n\n=p &>o\n\nthrrotol > O\n\n. oO X<O 06\nFem = a 9620.6\n\n0.0 2.5\n\nBinary Step Activation Function\n\ne Binary step function depends on a threshold value that decides whether a neuron should\nbe activated or not.\n\ne The input fed to the activation function is compared to a certain threshold; if the input is\ngreater than it, then the neuron is activated, else it is deactivated, meaning that its output\n\nis not passed on to the next hidden layer.\nHere are some of the limitations of binary step function:\n\ne It cannot provide multi-value outputs\u2014for example, it cannot be used for multi-class\nclassification problems.\nThe gradient of the step function is zero, which causes a hindrance in the backpropagation\nprocess.",
  "Page_22": "LINEAR ACTIVATION FUNCTION\n\n \n\n4 2 o 2\nx\n\nLinear Activation Function\n\ne The linear activation function, also known as \"no activation,\" or \"identity function\"\n(multiplied x1.0), is where the activation is proportional to the input.\ne The function doesn't do anything to the weighted sum of the input, it simply spits out\n\nthe value it was given.\nHowever, a linear activation function has two major problems :\n\ne It's not possible to use backpropagation as the derivative of the function is a constant\nand has no relation to the input x.\n\ne All layers of the neural network will collapse into one if a linear activation function is\nused. No matter the number of layers in the neural network, the last layer will still be\na linear function of the first layer. So, essentially, a linear activation function turns the\nneural network into just one layer.",
  "Page_23": "NON-LINEAR ACTIVATION FUNCTION\n\nBecause of its limited power, linear does not allow the model to create complex mappings\n\nbetween the network's inputs and outputs.\nNon-linear activation functions solve the following limitations of linear activation functions:\n\ne They allow backpropagation because now the derivative function would be related to the\ninput, and it's possible to go back and understand which weights in the input neurons can\nprovide a better prediction.\n\nThey allow the stacking of multiple layers of neurons as the output would now be a\nnon-linear combination of input passed through multiple layers. Any output can be\nrepresented as a functional computation in a neural network.\n\nSigmoid Activation Function\n\n \n\n\u2014 sigmoid\n\u2014 gradient\n\n1\n\nI=\n\nFollowing chain rule to find derivative\n\n \n\n \n\n \n\nfx) = \u201c a fa)=\u2014| fase ofa) =e\n\nx -100 -75 -50 -25 00 25 50 75\nx\n\n \n\n \n\n \n\n \n\ne*\n\n= \u2014\u2014__ = \u2014_*\n(+e tey \u201c\n\n\u201cx __ | = -\nf(a) = (-e\") Tae) =fO)*0-F00)\n\n-| x\n(+e)",
  "Page_24": "It is commonly used for models where we have to predict the probability as an\noutput. Since probability of anything exists only between the range of 0 and 1,\nsigmoid is the right choice because of its range.\n\nThe function is differentiable and provides a smooth gradient, i.e., preventing jumps\nin output values. This is represented by an S-shape of the sigmoid activation\nfunction.\n\nDrawbacks: *\n\n1,\n\nAs the gradient value approaches zero, the network ceases to learn and suffers\n\nfrom the Vanishing gradient problem.\n\nThe output of the logistic function is not symmetric around zero. So the output of all\nthe neurons will be of the same sign. This makes the training of the neural network\nmore difficult and unstable.\n\n2. tan-h\n\ntanh function\n\nTanh function is very similar to the sigmoid/logistic activation function, and even has\nthe same S-shape with the difference in output range of -1 to 1.\n\nAdvantages of using this activation function are:\n\ne The output of the tanh activation function is Zero centered; hence we can easily\nmap the output values as strongly negative, neutral, or strongly positive.\nUsually used in hidden layers of a neural network as its values lie between -1 to\n1; therefore, the mean for the hidden layer comes out to be 0 or very close to it.\nIt helps in centering the data and makes learning for the next layer much easier.\n\nDrawback:\n\nit also faces the problem of vanishing gradients similar to the sigmoid activation\nfunction.",
  "Page_25": "ReLU \u2014 Rectified Linear Unit\n\nReLU, short for rectified linear unit, is a non-linear activation function used\nfor deep neural networks in machine learning. It is also known as the rectifier\nactivation function. It helps in mitigate the vanishing gradient problem during\nmachine learning model training and enabling neural networks to learn more\ncomplex relationships in data.\n\nIf a model input is positive, the ReLU function outputs the same value. If a\nmodel input is negative, the ReLU function outputs zero.\n\ne ReLu helps models to learn faster and it's performance is better\n\nNonlinearity isa statistical term that describes a relationship between\nvariables that is not linear, or cannot be expressed with a straight line\n\nRectified Linear Unit (ReLU)\nActivation Function\n\n \n\n\u2014 relu\ngradient\n\nf(x) = max {0, x}\n\n \n\n \n\n \n\nf'(x)=1, x>=0\n=0, x<0O\n\ne ReLU stands for Rectified Linear Unit. \u00bb\n\n\u00a2 Although it gives an impression of a linear function, ReLU has a derivative function and allows for\nbackpropagation while simultaneously making it computationally efficient.\nThe main catch here is that the ReLU function does not activate all the neurons at the same time.\nThe neurons will only be deactivated if the output of the linear transformation is less than 0.\nThe advantages of using ReLU as an activation function are as follows:\n\nb\nSince only a certain number of neurons are activated, the ReLU function is far more\n\ncomputationally efficient when compared to the sigmoid and tanh functions.\nReLU accelerates the convergence of gradient descent towards the global minimum of the loss\nfunction due to its linear, non-saturating property.",
  "Page_26": "Disadvantages\n1, Itis not zero-centred, making training slightly unstable and requiring more iterations to train on for better\nperformance.\n\n. It completely removes the negative values from the calculation making those neurons inactive. Suppose a cas4\nwhere the weight multiplication and bias addition will always output a negative value irrespective of the input\n\nvalues. Here, the entire network architecture will fall into a dead state, known as dead relu.\n\n. ReLU is unbounded, which helps solve the vanishing gradient problem, but it becomes prone to exploding\n\ngradient issues if weight or bias values have a higher magnitude.\nHeuristics for Avoiding Bad Local Minima\n\nAvoiding bad local minima in deep learning is especially important because\ndeep neural networks often involve highly non-convex loss landscapes. Here are\nkey heuristics tailored for deep learning:\n\n1. Careful Weight Initialization\ne Proper initialization can help networks converge to better solutions.\n\no Xavier Initialization: Scales weights based on the number of\ninput/output neurons.\n\nHe Initialization: Suitable for networks with ReLU activations.\n\nOrthogonal Initialization: Ensures diversity in the initial weights.",
  "Page_27": "2. Optimizers with Momentum\n\ne Use advanced optimization algorithms that incorporate momentum or\nadaptive learning rates:\n\no SGD with Momentum: Helps accelerate convergence and escape\nflat minima.\n\nAdam: Combines momentum with adaptive learning rates to\nimprove optimization stability.\n\nRMSProp: Adjusts the learning rate for each parameter based on\nrecent gradients.\n\n3. Learning Rate Scheduling\n\ne Adjust the learning rate dynamically during training:\no Exponential Decay: Gradually reduces the learning rate over time.\n\nCosine Annealing: Cyclically reduces and resets the learning rate\nto avoid shallow minima.\n\nWarm Restarts: Temporarily increase the learning rate to escape\npotential traps.\n\n4. Stochastic Gradient Descent (SGD)\n\n\u00a2 The stochastic nature of mini-batch SGD introduces noise into gradient\nupdates, which can help escape bad local minima.\n\n5. Batch Normalization\ne Normalize intermediate activations to stabilize the training process.\n\ne It smooths the loss landscape and allows higher learning rates, reducing\nthe likelihood of bad minima.\n\n6. Over-Parameterization\n\ne Use larger networks with more parameters. Over-parameterized models\noften lead to smoother loss landscapes with fewer bad local minima.\n\n7. Regularization\ne Encourage generalizable solutions:\n\no L2 Regularization (Weight Decay): Penalizes large weights,\nsmoothing the loss landscape.\n\n27",
  "Page_28": "o Dropout: Randomly drops units during training, preventing\nreliance on specific paths.\n\n8. Skip Connections\n\n\u00a2 Modern architectures like ResNets and DenseNets incorporate skip\nconnections, which help smooth loss landscapes and make optimization\neasier.\n\n9. Data Augmentation\n\n- Diversify the training dataset with augmented samples. This can prevent\nthe model from overfitting to bad minima.\n\n10. Loss Function Design\n\u00a2 Use loss functions that are less likely to cause optimization traps:\n\no Label Smoothing: Adds noise to the target labels, preventing\noverconfidence.\n\nAuxiliary Losses: Add additional loss terms (e.g., from\nintermediate layers) to guide optimization.",
  "Page_29": "11. Gradual Model Scaling\n\n\u00a2 Train on simpler tasks or smaller models and progressively increase\ncomplexity (e.g., curriculum learning or progressive growing of\nnetworks).\n\n12. Perturbations\n- Inject noise into the training process:\n\nco Add noise to gradients or weights to encourage exploration of the\nloss landscape.\n\nGradient Noise Injection: Stochastically perturbs gradients during\nupdates.\n\n13. Transfer Learning\n\n- Fine-tune pre-trained models instead of training from scratch. Pre-trained\nweights often start closer to good minima.\n\n14. Ensemble Methods\n\n\u00a2 Train multiple models and ensemble their outputs. This mitigates the\nimpact of any single model being trapped in a bad minimum.\n\n15. Visualization and Analysis\n- Use tools like:\nco t-SNE or PCA: Visualize embeddings and optimization progress.\n\nco Loss Surface Visualization: Assess how smooth or rugged the loss\nlandscape is.\n\nBy combining these heuristics, deep learning practitioners can improve the\nrobustness of training and avoid getting stuck in bad local minima,\nultimately achieving better generalization and performance.\n\nHeuristics for Faster Training\n\nFaster training in deep learning can significantly reduce computational costs and|\nspeed up model development. Below are heuristics and techniques to achieve\nfaster training:\n\n1. Efficient Data Handling\n1. Data Preprocessing:\no Normalize or standardize input data to improve convergence.\n\no Use data augmentation during preprocessing to enhance\ngeneralization without additional training data.\n\n2. Data Pipeline Optimization:\n\no Use frameworks like TensorFlow Data API or PyTorch\nDataLoader for efficient data loading.\n\nPrefetch, parallelize, and batch data loading to prevent I/O\nbottlenecks.\n\n3. Mixed Precision Training:\n\no Use lower-precision (e.g., FP 16) computations instead of FP32\nwhere supported by hardware, like NVIDIA GPUs with Tensor\nCores.\n\n29",
  "Page_30": "2. Optimizers\n1. Momentum-Based Optimizers:\n\no Use optimizers like Adam, RMSProp, or SGD with Momentum\nfor faster convergence.\n\n2. Learning Rate Scheduling:\n\no Use learning rate decay methods like:\n\n= Step Decay: Reduce learning rate at specific epochs.\n\nCosine Annealing: Smoothly adjust learning rates in cycles.\n\nWarm Restarts: Temporarily increase learning rate to\nescape plateaus.\n\n3. Model Architecture Design\n1. Efficient Architectures:\n\no Use modern, optimized architectures like MobileNet,\nEfficientNet, or ResNet.\n\no Consider pruning or using lightweight layers (e.g., depthwise\nseparable convolutions).\n\n2. Skip Connections:\n\nco Incorporate residual or skip connections (e.g., ResNet) to reduce\nvanishing gradient issues and accelerate training.\n\n4. Training Techniques\n1. Batch Size Optimization:\n\no Use the largest batch size that fits in memory. Larger batches\ntypically lead to faster convergence.\n\n2. Gradient Accumulation:\n\no If memory is limited, accumulate gradients over smaller batches to\nsimulate a larger effective batch size.\n\n3. Gradient Clipping:\n\no Clip gradients to prevent exploding gradients, enabling faster\nconvergence without destabilizing updates.\n\n4. Knowledge Distillation:\n\no Usea pre-trained \"teacher\" model to guide the \"student\" model for\nfaster convergence.",
  "Page_31": "5. Regularization and Early Stopping\n1. Dropout and Weight Decay:\n\no Regularize training to prevent overfitting, reducing the need for\nlong training times.\n\n2. Early Stopping:\n\no Monitor validation loss and stop training once it stops improving,\navoiding unnecessary epochs.\n\n6. Transfer Learning\n\ne Fine-tune a pre-trained model instead of training from scratch,\nsignificantly reducing the number of epochs required.\n\n7. Parallelism and Hardware Utilization\n1. GPU/TPU Utilization:\n\no Use hardware accelerators like GPUs or TPUs for parallel\ncomputation.\n\n2. Multi-GPU Training:\n\no Distribute training across multiple GPUs using frameworks like\nPyTorch Distributed Data Parallel or TensorFlow Mirrored\nStrategy.\n\n3. Distributed Training:\n\no Leverage cluster-based training for large-scale models or datasets.\n\n8. Hyperparameter Tuning\n\n1. Automated Search:\n\no Use tools like Optuna, Ray Tune, or Hyperopt to find optimal\nhyperparameters quickly.\n\n2. Learning Rate Finder:\n\no Use methods like the learning rate range test to determine the best\nlearning rate.",
  "Page_32": "9. Checkpoints and Resuming\n\n\u00a2 Save and resume training checkpoints to avoid restarting from scratch i\ncase of interruptions.\n\n10. Profiling and Debugging\n1. Profiling Tools:\n\no Use tools like NVIDIA Nsight, TensorFlow Profiler, or PyTorch\nProfiler to identify bottlenecks.\n\n2. Debugging and Visualization:\n\nUse frameworks like TensorBoard to monitor loss, accuracy, and other metrics\nto diagnose inefficiencies\n\n11. Pruning and Compression\n1. Weight Pruning:\no Remove unimportant weights to reduce model complexity and training time.\n\n2. Quantization:\n\no Use quantized operations during training or inference for faster computations.\n\n12. Avoid Overtraining\n\n\u00a2 Monitor training metrics and validation performance closely. Avoid training too long\nto minimize unnecessary compute time.\n\nBy implementing these heuristics, which can effectively reduce the time required to train\ndeep learning models while maintaining or improving their performance.",
  "Page_33": "Nesterov's Accelerated Gradient Descent (NAG):\n\nGradient descent\n\nIt is essential to understand Gradient descent before we look at Nesterov Accelerated\nAlgorithm. Gradient descent is an optimization algorithm that is used to train our model.\nThe accuracy of a machine learning model is determined by the cost function. The lower the\ncost, the better our machine learning model is performing. Optimization algorithms are used\nto reach the minimum point of our cost function. Gradient descent is the most common\noptimization algorithm. It takes parameters at the start and then changes them iteratively to\nreach the minimum point of our cost function.\n\nInitial\n\nWeight / Gradient\nwe /\na\n\nIncremental\n\nStep NY\n\na\n<_< Minimum Cost\n\nDerivative of Cost\n\nwe take some initial weight, and according to that, we are positioned at some\npoint on our cost function. Now, gradient descent tweaks the weight in each\niteration, and we move towards the minimum of our cost function accordingly.\n\n    \n\nmodel is very important as it can cause problems while training.\n\nA low learning rate assures us to reach the minimum point, but it takes a lot o\niterations to train, while a very high learning rate can cause us to cross the\nminimum point, a problem commonly known as overshooting.\n\nThe size of our steps depends on the learning rate of our model. The higher the\nlearning rate, the higher the step size. Choosing the correct learning rate for or",
  "Page_34": "Too low Just right Too high\n\nAsmall learning rate The optimal learning\nrequires many updates rate swiftly reaches the\nbefore reaching the minimum point\nminimum point\n\nToo large of a learning rate\ncauses drastic updates\nwhich lead to divergent\n\n* behaviors\n\nDrawbacks of gradient descent\n\nThe main drawback of gradient descent is that it depends on the learning rate and the\ngradient of that particular step only. The gradient at the plateau, also known as saddle\npoints of our function, will be close to zero. The step size becomes very small or even zero.\nThus, the update of our parameters is very slow at a gentle slope.\n\nLet us look at an example. The starting point of our model is \u2018A\u2019. The loss function will\ndecrease rapidly on the path AB because of the higher gradient. But as the gradient\ndecreases from B to C, the learning is negligible. The gradient at point \u2018C\u2019 is zero, and it is the\nsaddle point of our function. Even after many iterations, we will be stuck at \u2018C\u2019 and will not\nreach the desired minimum \u2018D\u2019.\n\nA\n\nD\n\nThis problem is solved by using momentum in our gradient descent.\n\n34",
  "Page_35": "Gradient descent with momentum\n\nThe issue discussed above can be solved by including the previous gradients in our\ncalculation. The intuition behind this is if we are repeatedly asked to go in a particular\ndirection, we can take bigger steps towards that direction.\n\nThe weighted average of all the previous gradients is added to our equation, and it acts a4\nmomentum to our step.\n\ncost\nMovement =\n\nNegative of Gradient + Momentum\n=> Negative of Gradient\n\nseee> Momentum\n\n=\u2014\u2014\u2014p> Real Movement\n\nGradient =0",
  "Page_36": "As we start to descend, the momentum increases, and even at gentle slopes\nwhere the gradient is minimal, the actual movement is large due to the added\nmomentum.\n\nBut this added momentum causes a different type of problem. We actually cross\nthe minimum point and have to take a U-turn to get to the minimum point.\nMomentum-based gradient descent oscillates around the minimum point, and\nwe have to take a lot of U-turns to reach the desired point. Despite these\noscillations, momentum-based gradient descent is faster than conventional\ngradient descent.\n\nTo reduce these oscillations, we can use Nesterov Accelerated Gradient.\n\nNAG resolves this problem by adding a look ahead term in our equation. The intuition behi\nNAG can be summarized as \u2018look before you leap\u2019. Let us try to understand this through an}\nexample.\n\nWiook_ahead\nWo\n\n(a) Momentum-Based Gradient Descent (b) Nesterov Accelerated Gradient Descent\n\nOL _ Negative(\u2014) OL _ Negative(\n\n=)\n= = 5 = 2 = 2 TY\n0 Owo \u2014_ Positive(+) 0 Owo Negative(\u2014)",
  "Page_37": "As can see, in the momentum-based gradient, the steps become larger and larger due to the\naccumulated momentum, and then we overshoot at the 4th step. We then have to take\nsteps in the opposite direction to reach the minimum point.\n\nHowever, the update in NAG happens in two steps. First, a partial step to reach the look-\nahead point, and then the final update. We calculate the gradient at the look-ahead point\nand then use it to calculate the final update. If the gradient at the look-ahead point is\nnegative, our final update will be smaller than that of a regular momentum-based gradient.\nLike in the above example, the updates of NAG are similar to that of the momentum-based\ngradient for the first three steps because the gradient at that point and the look-ahead point\nare positive. But at step 4, the gradient of the look-ahead point is negative.\n\nIn NAG, the first partial update 4a will be used to go to the look-ahead point and then the\ngradient will be calculated at that point without updating the parameters. Since the gradient\nat step 4b is negative, the overall update will be smaller than the momentum-based\ngradient descent.\n\nWe can see in the above example that the momentum-based gradient descent takes six\nsteps to reach the minimum point, while NAG takes only five steps.\n\nThis looking ahead helps NAG to converge to the minimum points in fewer steps and reduce\nthe chances of overshooting.\n\nHow NAG Works\n\nWe saw how NAG solves the problem of overshooting by \u2018looking ahead\u2019. Let us see how\nthis is calculated and the actual math behind it.\n\nUpdate rule for gradient descent:\n\nWee = We - Vt\n\nIn this equation, the weight (W) is updated in each iteration. n is the learning rate, and Vwt\nis the gradient.\n\nUpdate rule for momentum-based gradient descent:\n\nIn this, momentum is added to the conventional gradient descent equation. The update\nequation is\n\nWei1 = Wy - update;\n\nupdate, is calculated by:\nupdate; = y - update;_; + nVw;y",
  "Page_38": "update =\n\nupdate - updatey + nVw, = nVui\n\nupdates = 7 - update, + nVwe = 7: nVw, + 7Vwe\n\nupdates - updates + nVw3 = \u00a5(y-7Vwi + nVwe) + 7Vw3\n- updateg + nVw3 = 77 -nVui + y-nVwe + nVw3\n\nupdate, - update; + nVwa \u201c3. nVwi + a. nVwe+7-nVw3+nVwa\n\nupdate, - updates; + nVur = -}. nVwi + ym, nVui +... + 1Vwre\n\nThis is how the gradient of all the previous updates is added to the current update.\n\nUpdate rule for NAG:\n\nWi+1 = Wy \u2014 update;\n\nWhile calculating the update;, We will include the look ahead gradient (VWiggk ahead):\nupdate; = y - updatey_1 + NVWigok_ahead\n\nVWiook_ahead is calculated by:\n\nWlook_ahead = Wt - Y* updatey_1\n\nThis look-ahead gradient will be used in our update and will prevent overshooting.\n\nPseudocode\n\nInitialize 6 (parameters), y (momentum), n (learning rate), and v (velocity vector)\nRepeat until convergence:\n1. Lookahead step: compute temporary parameters\n@_lookahead = 6 - y * v\n. Compute gradient at lookahead position\ng = Vf(6_lookahead)\n. Update velocity using gradient\nvey*vtn*g\n. Update parameters using velocity\n\n@=@-v",
  "Page_39": "Key Features\n1. Lookahead Gradient:\n\ne NAG anticipates the future position of the parameters based on momentum before\n\ncomputing the gradient.\ne This leads to more accurate updates, especially near minima.\n2. Faster Convergence:\ne The correction term helps prevent overshooting and improves convergence speed.\n3. Smoothness:\n\ne NAG provides smoother parameter updates compared to standard momentum methods.\n\nAdvantages\n1. Improved Accuracy:\ne By accounting for the momentum\u2019s effect, NAG often finds better minima.\n2. Better Near Minima:\ne Itis particularly effective at fine-tuning when the optimization is near a minimu\n3. General Applicability:\n\n\u00a2 Can be used with non-convex loss functions common in deep learning.\n\nComparison with Momentum\n\nAspect Momentum NAG\n\nGradient Location Current position Lookahead position\n\nUpdate Behavior Can overshoot minima Anticipates future updates, reducing overshooting\nConvergence Speed Slower in some cases Faster convergence\n\nAccuracy Near Minima Less precise More precise\n\nIn frameworks like TensorFlow and PyTorch, NAG can be implemented easily\n\n39",
  "Page_40": "import torch\n\nimport torch.optim as optim\n\nModel =...#define your model\n\nOptimizer =optim.SGD(model.parameters(),lr=0.01, momentum=0.9,nesterov=True)\n\nApplications\n1. Training deep neural networks with high-dimensional parameter spaces.\n2. Accelerating convergence for convex and non-convex optimization problems.\n\n3. Reducing oscillations in optimization paths.\n\n   \n\nNesterov's Accelerated Gradient Descent is a robust and efficient tool, making it a popular choice fqr\n\ndeep learning practitioners.\n\n2.3 Regularization\nA fundamental problem in machine learning is how to make an\nalgorithm that will perform well not just on the training data, but also onnew\ninputs. Many strategies used in machine learning are explicitly designed to\nreduce the test error, possibly at the expense of increased training error.\nThese strategies are known collectively as regularization.\n\nDefinition: - \u201cany modification we make to a learning algorithm that is\nintended to reduce its generalization error but not its training error.\u201d\n\no\n\n\u201c+ In the context of deep learning, most regularization strategies are\nbased on regularizing estimators.\n\no\n\n** Regularization of an estimator works by trading increased bias for\nreduced variance.\n\nAn effective regularizer is one that makes a profitable trade, reducing\nvariance significantly while not overly increasing the bias.\n\nMany regularization approaches are based on limiting the capacity of models,\nsuch as neural networks, linear regression, or logistic regression, by adding a\nparameter norm penalty Q(8) to the objective function J. We denote the\nregularized objective function by J~\n\nJO; X, y) = (0; X, y) + dQ)\n\nwhere a \u20ac [0, 00) is a hyperparameter that weights the relative contribution of\nthe norm penalty term, Q, relative to the standard objective function J. Setting a to\n0 results in no regularization. Larger values of o correspond to more regularization.\n\n40",
  "Page_41": "\u201c+ The parameter norm penalty Q that penalizes only the weights of the affine\ntransformation at each layer and leaves the biases unregularized.\n\n2.3.1 L2 Regularization\nOne of the simplest and most common kind of parameter norm penalty is L2\n\nparameter & it\u2019s also called commonly as weight decay. This regularization strategy\ndrives the weights closer to the origin by adding a regularization term\n\nm2(e8)= %|[|w ll.\n\nDegree 4 Degree 15\n\n\u2014 Mode!\n\u2014\u2014 True function\n\n\u2014\u2014 Model\n\u2014\u2014 True function\n\ne Samples \u00a9 Samples\n\nGood Fit High Variance\n\nL2 regularization is also known as ridge regression or Tikhonov regularization. To\nsimplify, we assume no bias parameter, so 0 is just w. Such a model has the\nfollowing total objective function.\n\nJ(w:X,y) = Sw Yaw + J(w;X,y),\nwith the corresponding parameter gradient\nVw (w; X,y) = aw + Vad (w; X,y). (\nTo take a single gradient step to update the weights, we perform this update\nwe\u2014w-\u2014e(awt+ VwJ(w;X,y)). (\nWritten another way, the update is\n\nw< (1l\u2014ca)w \u2014eVwJ(w: X.y).\n\n\u201c* We can see that the addition of the weight decay term has modified the learning\nrule to multiplicatively shrink the weight vector by a constant factor on each step,\n\njust before performing the usual gradient update. This describes what happens in a\nsingle step.\n\n\u201c+ The approximation AJ\nby\n\nGiven",
  "Page_42": "J(@) J (w*) * ( aw*) | FA(w \u2014 w*).\n\nWhere H is the Hessian matrix of J with respect to w evaluated at w+.",
  "Page_43": "The minimum of *J occurs where its gradient VwJ(w) = H(w \u2014 w*) is equal to \u20180\u2019\n\nTo study the eff ect of weight decay,\n\now + Fl (w \u2014 w*) =o\n(FF + aF)w = Fw*\nap \u2014 (AF + at) ) Aw\n\nAs a approaches 0, the regularized solution ~w approaches w*. But what happens as a grows?\nBecause H is real and symmetric, we can decompose it into a diagonal matrix A and an\northonormal basis of eigenvectors, Q, such that H = QAQ\u2019. Applying Decomposition to the\nabove equation, We Obtain\n\na0 (QA! + at) 1QAQ! ww\n[QA + ang! ] QAQ!' w*\nQCA + aT) 1AQ! w*.\n\nFigure 2: Weight updation effect\n\nThe solid ellipses represent contours of equal value of the unregularized objective. The dotted\ncircles represent contours of equal value of the L 2 regularizer. At the point w~, these competing\nobjectives reach an equilibrium. In the first dimension, the eigenvalue of the Hessian of J is small.\nThe objective function does not increase much when moving horizontally away from w* . Because\nthe objective function does not express a strong preference along this direction, the regularizer has a\nstrong effect on this axis. The regularizer pulls w1 close to zero. In the second dimension, the\nobjective function is very sensitive to movements away from w* . The corresponding eigenvalue is\nlarge, indicating high curvature. As a result, weight decay affects the position of w2 relatively little.",
  "Page_44": "L2 Regularization\n\nA linear regression that uses the L2 regularization technique is called ridge regression. In other words, in ridge\nregression, a regularization term is added to the cost function of the linear regression, which keeps the\nmagnitude of the model's weights (coefficients) as small as possible. The L2 regularization technique tries to\nkeep the model's weights close to zero, but not zero, which means each feature should have a low impact on the\noutput while the model's accuracy should be as high as possible.\n\nl m\n2\n3 AY wi 4\n\nrl\n\nRidge Regression Cost Function = Loss Function +\n\nWhere \\ controls the strength of regularization, and w, are the model's weights (coefficients).\n\nBy increasing A, the model becomes flattered and underfit. On the other hand, by decreasing A, the model\nbecomes more overfit, and with A = 0, the regularization term will be eliminated.\n\n \n\n2.3.2 L1 Regularization\n\nWhile L2 weight decay is the most common form of weight decay, there are other ways to\npenalize the size of the model parameters. Another option is to use L1 regularization.\n\nL1 Regularization\n\nLeast Absolute Shrinkage and Selection Operator (/asso) regression is an alternative to ridge for regularizing\nlinear regression. Lasso regression also adds a penalty term to the cost function, but slightly different, called L1\nregularization. L1 regularization makes some coefficients zero, meaning the model will ignore those features.\nIgnoring the least important features helps emphasize the model's essential features.\n\nm\nLasso Regression Cost Function = Loss Function + 1y> wy\njo\n\nWhere \\ controls the strength of regularization, and w, are the model's weights (coefficients).\n\nLasso regression automatically performs feature selection by eliminating the least important features.",
  "Page_45": ">  L1 regularization on the model parameter w is defined as the sum of absolute values of the\nindividual parameters.\n\nQ(@) = |\\ew| la = SF Jewel.\nL1 weight decay controls the strength of the regularization by scaling the penalty Q using a\n\npositive hyperparameter a. Thus, the regularized objective function J\u00b0(w; X, y) is given by\n\nJ(w;X,y) =a||w||i + J(w; X,y),\nwith the corresponding gradient as\n\nVw) (w; X,y) = asign(w) + Vwd(X,y;w), \u2014\u2014- fal\n\nBy inspecting equation 1, we can see immediately that the effect of L 1 regularization is quite\ndifferent from that of L 2 regularization. Specifically, we can see that the regularization\ncontribution to the gradient no longer scales linearly with each wi ; instead it is a constant factor\nwith a sign equal to sign(wi).\n\nQuadratic approximation of the L 1 regularized objective function decomposes into a sum over the parameters\n\nJ(w: X,y) = J(w*; X,y) + >  jHis(w \u2014wry)+ aluil :\n\nThe problem of minimizing this approximate cost function has an analytical solution with the following form:\n\ns a\nw; = sign(w; ) max \u00a2 |w;| \u2014 7 50>.\na2\n\nConsider the situation where w * i > 0 for all i. There are two possible outcomes:\n\n1. The case where w} < Ha: Here the optimal value of w; under the regularized\nobjective is simply w; = 0. This occurs because the contribution of J (w; X,y)\nto the regularized objective J (w;X,y) is overwhelmed\u2014in direction i\u2014by\nthe L! regularization, which pushes the value of w; to zero.\n\n. The case where w} > 7-. In this case, the regularization does not move the\n\noptimal value of w; to zero but instead just shifts it in that direction by a\ndistance equal to 7-.",
  "Page_46": "Elastic Net Regularization\n\nThe third type of regularization,(you may have guessed by now) uses both\nmost optimized output\nIn addition to setting and choosing a lambda value elastic net also allows us to tune the alpha parameter where a\n\norresponds to ridge and a = 1 to lasso, Simply pul, if you plug in 0 for alpha, the penalty function reduces to\n\nthe L1 (ridge) term and if we set alpha to 1 we get the L2 (lasso) term\n\nCost function of Elastic Net Regularization\n\nJ( 8), Be, \u00ab+++\u00bb Bm) Vly 3 iP +Ma >> 3,|4 sey 3?)\n\nTherefore we can choose an alpha value between 0 and 1 to optimize the elastic net(here we can adjust the\n\nnlage of each regularization,thus giving the name elastic). Effectively this will shrink some coefficients and set\n\n\u00a9 0 for sparse selection\n\n2.3.3 Difference between L1 & L2 Parameter Regularization\n\n> L1 regularization attempts to estimate the median of data, L2 regularization makes estimation\nfor the mean of the data in order to evade overfitting.",
  "Page_47": "L1 regularization can add the penalty term in cost function. But L2 regularization appends the\nsquared value of weights in the cost function.\n\nL1 regularization can be helpful in features selection by eradicating the unimportant features,\nwhereas, L2 regularization is not recommended for feature selection\n\nL1 doesn\u2019t have a closed form solution since it includes an absolute value and it is a non-\ndifferentiable function, while L2 has a solution in closed form as it\u2019s a square of a weight\n\n \n\nS.No\n\nL1 Regularization\n\nL2 Regularization\n\n \n\nPanelizes the sum of absolute\nvalue of weights.\n\npenalizes the sum of square weights.\n\n \n\nIt has a sparse solution.\n\nIt has a non-sparse solution.\n\n \n\nIt gives multiple solutions.\n\nIt has only one solution.\n\n \n\nConstructed in feature selection.\n\nNo feature selection.\n\n \n\nRobust to outliers.\n\nNot robust to outliers.\n\n \n\nIt generates simple and\ninterpretable models.\n\nIt gives more accurate predictions when the output\nvariable is the function of whole input variables.\n\n \n\nUnable to learn complex data\npatterns.\n\nAble to learn complex data patterns.\n\n \n\n \n\n \n\nComputationally inefficient over\nnon-sparse conditions.\n\n \n\nComputationally efficient because of having\nanalytical solutions.\n\n \n\nEarly Stopping\n\nEarly stopping is a kind of cross-validation strategy where we keep one part of the training set as the\n\nvalidation set. When we see that the performance on the validation set is getting worse, we immediately\n\nstop the training on the model. This is known as early stopping.\n\nIn the above image, we will stop training at the dotted line since after that our model will start overfitting\n\non the training data.",
  "Page_48": "2.4 Batch Normalization:\n\nIt is a method of adaptive reparameterization, motivated by the difficulty of training\nvery deep models.In Deep networks, the weights are updated for each layer. So the output\nwill no longer be on the same scale as the input (even though input is\nnormalized).Normalization - is a data pre-processing tool used to bring the numerical data to\na common scale without distorting its shape.when we input the data to a machine or deep\nlearning algorithm we tend to change the values to a balanced scale because, we ensure that\n\nour model can generalize appropriately.(Normalization is used to bring the input into a\nbalanced scale/ Range).\n\nLet's understand this through an example, we have a deep neural network as shown in the following image.",
  "Page_49": "Initially, our inputs X1, X2, X3, X4 are in normalized form as they are coming from the pre-processing stage. When\nthe input passes through the first layer, it transforms, as a sigmoid function applied over the dot product of input\nX and the weight matrix W.\n\nxX, On j@\n\nXx\n\n-\nW, \u2018eo -\n\nh, = o(W,X)\n\nx h, = o(W,h,) = o(W,0(W,X))\nNormalize the inputs . \u00b0 \u00b0\n\nO = a(W,h, ,)\n\nwea\n\nImage Source: https://www.analyticsvidhya.com/blog/2021/03/introduction-to-batch-normalization/\n\nEven though the input X was normalized but the output is no longer on the same scale. The\ndata passes through multiple layers of network with multiple times(sigmoidal) activation functions\nare applied, which leads to an internal co-variate shift in the data.\n\nThis motivates us to move towards Batch Normalization\n\nNormalization is the process of altering the input data to have mean as zero and standard deviation\nvalue as one.\n\n2.4.1 Procedure to do Batch Normalization:\n\n(1) Consider the batch input from layer h, for this layer we need to calculate the mean of this hidden\nactivation.\n(2) After calculating the mean the next step is to calculate the standard deviation of the hidden\nactivations.\n(3) Now we normalize the hidden activations using these Mean & Standard Deviation values. To do\nthis, we subtract the mean from each input and divide the whole value with the sum of standard\ndeviation and the smoothing term (\u00a2).\n(4) As the final stage, the re-scaling and offsetting of the input is performed. Here two components\nof the BN algorithm is used, y(gamma) and f (beta). These parameters are used for re-scaling (y) and\nshifting(B) the vector contains values from the previous operations.\n\nThese two parameters are learnable parameters, Hence during the training of neural network,\nthe optimal values of y and \u00a3 are obtained and used. Hence we get the accurate normalization of each\nbatch.",
  "Page_50": "2.5. Shallow Networks\nShallow neural networks give us basic idea about deep neural network which consist\nof only 1 or 2 hidden layers. Understanding a shallow neural network gives us an\nunderstanding into what exactly is going on inside a deep neural network A neural network is\nbuilt using various hidden layers. Now that we know the computations that occur in a\nparticular layer, let us understand how the whole neural network computes the output for a\ngiven input X. These can also be called the forward-propagation equations.\n\nZN = Wurx + pl\nAllag (Z!4)\nZ2\\ \u2014 weir All + pl\n\n= AM = (22)\n\n1 The first equation calculates the intermediate output Z/fof the first hidden layer.\n\n2, The second equation calculates the final outout A/jof the first hidden layer.\n\n3, The third equation calculates the intermediate output Z[2Jof the output layer.\n\n4, The fourth equation calculates the final output A/2Jof the output layer which is also the final\noutput of the whole neural network.\n\nShallow-Deep Networks: A Generic Modification to Deep Neural Networks\n\nInternal Layers Final Classifier\n\n-\u2014-7TTT_+1_11__\n\nconv1 conv2 conv3 conv4 Final ;\nPrediction\n\nr-L-; r-t-s,\n\nl 1\n\u2018FR; \\ FR, [conv: Convolutional layer\n\n1!\nInternal\nClassifier i\n(IC) 1 full , .\not : FR: Feature Reduction layer\n7 :\nInternal Internal\nPrediction Prediction\n\n-- \u00a5 ull: Fully connected layer\n\nFigure 2:Shallow Networks \u2014 Generic Model\n\n50",
  "Page_51": "2.5.1 Difference Between a Shallow Net & Deep Learning Net:\n\nShallow Net\u2019s\n\nOne Hidden layer(or very less no. of\nHidden Layers)\n\nTakes input only as VECTORS\n\nShallow net\u2019s needs more parameters\nto have better fit\n\nShallow networks with one Hidden\nlayer (same no of neurons as DL)\ncannot place complex functions over\nthe input space\n\nThe number of units in a shallow\nnetwork grows exponentially with\ntask complexity.\n\nShallow network is more difficult to\ntrain with our current algorithms (e.g.\nit has issues of local minima etc)\n\nDeep Learning Net\u2019s\n\nDeep Net\u2019s has many layers of Hidden\nlayers with more no. of neurons in\neach layers\n\nDL can have raw data like image, text\nas inputs\n\nDL can fit functions better with less\nparameters than a shallow network\n\nDL can compactly express highly\ncomplex functions over input space\n\nDL don\u2019t need to increase it\nsize(neurons) for complex problems\n\nTraining in DL is easy and no issue of\nlocal minima in DL\n\n \n\nDropout\n\na,\n\nx oH t ae a\n\n\\\n\n% MK \u201c ae ey\n\n(a) Standard Neural Network\n\nat\n) ntl\nad\n\n(b) Network after Dropout",
  "Page_52": "Drawbacks of Dropout:\n\n. Increased Training Time: Dropout increases the training time of the neural network, as the\nnetwork needs to be trained multiple times with different subsets of neurons dropped\nout. However, this can be mitigated by parallelizing the training process.\n\n. Reduced learning rate: The use of dropout can reduce the effective learning rate of the\nnetwork, which can slow down the learning process.\n\n. Can cause instability: In some cases, dropout can cause instability during training,\nparticularly if the dropout rate is too high. This can be addressed by tuning the dropout\nrate and adjusting other hyperparameters.\n\n. Cannot be used with all types of networks: Dropout is not suitable for all types of neural\nnetworks, particularly those with a small number of neurons or those with a small number\nof layers.\n\nAdversial Training:\n\nAdversarial training is a technique used to improve the robustness\nand performance of machine learning models, particularly neural\nnetworks, by training them on adversarial examples.\n\nAdversarial examples are inputs modified by small, carefully\ncalculated perturbations that lead to incorrect outputs from the\nmodel. For example:\n\nIn image classification, a small noise addition might cause the\nmodel to misclassify an image of a cat as a dog.\n\nThese perturbations are usually computed to maximize the\nmodel's loss for a given input.",
  "Page_53": "Optimization\n* Deep learning relies on optimization methods.\n* The training efficiency of the model is directly influenced by the\noptimization algorithm's performance.\n\n* Understanding the fundamentals of different optimization algorithms\nand the function of their hyperparameters, on the other hand, will\nallow us to modify hyperparameters in a targeted manner to improve\ndeep learning model performance.\n\n* In simple words, Optimization algorithms are responsible for reducing\nlosses and provide most accurate results possible.\n\n\u00a2 The weight is initialized using some initialization strategies and is\nupdated with each epoch according to the equation.\n\n* The best results are achieved using some optimization strategies or\nalgorithms called Optimizer.\n\nThe goal of Optimization in Deep learning\n\n\u00a2 Although optimization may help deep learning by lowering the loss\nfunction, the aims of optimization and deep learning are fundamentally\ndifferent.\n\n\u00a2 The former is more focused on minimizing an objective, whereas the\nlatter is more concerned with finding a good model given a finite\n\nquantity of data.\n\n\u00a2 Training error and generalization error, for example, vary in that the\noptimization algorithm's objective function is usually a loss function\nbased on the training dataset, and the purpose of optimization is to\nminimize training error.\n\n* Deep learning (or, to put it another way, statistical inference) aims to\ndecrease generalization error. In order to achieve the latter, we must be\naware of overfitting as well as use the optimization procedure to lower\nthe training error.",
  "Page_54": "Some of the Optimization techniques:\n\u00a2 Gradient Descent Deep Learning Optimizer\n\u00a2 Stochastic Gradient Descent Deep Learning Optimizer\n\u00a2 Mini-batch Stochastic Gradient Descent\n\u00a2 Adagrad(Adaptive Gradient Descent) Optimizer\n\u00a2 RMSprop (Root Mean Square) Optimizer\n\u00a2 Adam Deep Learning Optimizer\n\u00a2 AdaDelta Deep Learning Optimizer\n\n1. Gradient Descent Deep Learning Optimizer\n\n* Gradient Descent is the most common optimizer in the class. Calculus is\nused in this optimization process to make consistent changes to the\nparameters and reach the local minimum.\n\nGradient descent works with a set of coefficients, calculates their cost, and\nlooks for a cost value that is lower than the current one. It shifts to a lesser\nweight and updates the values of the coefficients. The procedure continues\nuntil the local minimum is found. A local minimum is a point beyond which\nit is impossible to go any farther.\n\nThis algorithm is apt for cases where optimal points cannot be found by\nequating the slope of the function to 0. For the function to reach minimum\nvalue, the weights should be altered. With the help of back propagation, loss\nis transferred from one layer to another and \u201cweights\u201d parameter are also\nmodified depending on loss so that loss can be minimized.\n\nCost function: 6=0\u2014a-VJ(8)\n\n\u00a2 As for Gradient Descent algorithm, the entire data set is loaded at a\ntime. This makes it computationally intensive. Another drawback is\nthere are chances the iteration values may get stuck at local minima or\nsaddle point and never converge to minima. To obtain the best\nsolution, the must reach global minima.\n\n \n\nJw, b) global maximum\n\nlocal maximum\n\nlocal minimum\n\nglobal minimum\n\n \n\n \n\n1 Ll L\n0.2 0.4 0.6 0.8",
  "Page_55": "* Concept: Updates model parameters in the direction of the steepest descent of the loss\n\nfunction.\n\n\u00a2 Formula:\n\u00a7=0-7n- VJ(8)\n\nwhere 77 is the learning rate and VJ(8) is the gradient of the loss.\n\n\u00a2 Limitation: Requires the entire dataset for each update, making it slow for large datasets.\n\n2. Stochastic Gradient Descent Deep Learning Optimizer\n\n\u00a2 On large datasets, gradient descent may not be the best solution. We use\nstochastic gradient descent to solve the problem. The word stochastic refers\nto the algorithm's underlying unpredictability. Instead of using the entire\ndataset for each iteration, we use a random selection of data batches in\nstochastic gradient descent. As a result, we only sample a small portion of\nthe dataset. The first step in this technique is to choose the starting\nparameters and learning rate. Then, in each iteration, mix the data at random\nto get an estimated minimum. When compared to the gradient descent\napproach, the path taken by the algorithm is full of noise since we are not\nusing the entire dataset but only chunks of it for each iteration.\n\nAs a result, SGD requires more iterations to attain the local minimum. The\noverall computing time increases as the number of iterations increases.\n\nHowever, even when the number of iterations is increased, the computation\ncost remains lower than that of the gradient descent optimizer. As a result, if\nthe data is large and the processing time is a consideration, stochastic\ngradient descent should be favored over batch gradient descent.\n\nConcept: Updates parameters using one data sample at a time, making it faster than regular\n\ngradient descent.\n\nFormula:\n\n6=8-n- VI(6;2;,yi)\n\nwhere (2;, yi) is a single data point.\n\n\u00a2 Limitation: Noisy updates can cause fluctuations, making convergence less stable.",
  "Page_56": "3. Mini-batch Stochastic Gradient Descent\n\n* MB-SGD is an extension of SGD algorithm. It overcomes the time-\nconsuming complexity of SGD by taking a batch of points / subset of\npoints from dataset to compute derivative.\n\n\u00a2 Because the method employs batching, all of the training data does\nnot need to be placed into memory, making the process more\nefficient.\n\n* In addition, the cost function in mini-batch gradient descent is noisier\nthan that in batch gradient descent but smoother than that in\nstochastic gradient descent.\n\n\u00a9 Concept: Combines the advantages of batch gradient descent and SGD by updating parameters\n\nusing small batches of data.\n\n\u00a9 Formula:\n\n1 n\nG=0-9-\u2014 VIO; 2i,%)\n\ni=1\nwhere n is the batch size.\n\n\u00a2 Advantage: Balances speed and stability, making it widely used in practice.\n\n4. Adagrad(Adaptive Gradient Descent) Optimizer\n\n\u00a2 Adaptive Gradient as the name suggests adopts the learning rateof\nparameters by updating it at each iteration depending on the position it is\npresent, 1.e- by adapting slower learning rates when features are occurring\nfrequently and adapting higher learning rate when features are infrequent.\n\n\u00a2 Technically it acts on learning rate parameter by dividing the learning rate\nby the square root of gamma, which is the summation of all gradients\nsquared.\n\n\u00a2 In the update rule, AdaGrad modifies the general learning rate N at each\nstep for all the parameters based on past computations. One of the biggest\ndisadvantages is the accumulation of squared gradients in the denominator.\nSince every added term is positive, the accumulated sum keeps growing\nduring the training. This makes the learning rate to shrink and eventually\nbecome small. This method is not very sensitive to master step size and also\nconverges faster.",
  "Page_57": "Concept: Adapts the learning rate for each parameter based on the history of gradients, giving\n\nlarger updates to less frequently updated parameters.\n\nUpdate Rule:\n\n6=0-\n\nwhere G is the sum of past squared gradients.\nAdvantage: Handles sparse data well.\n\nLimitation: Learning rate may decay too much over time.\n\n5. AdaDelta\n\n\u00a2 It is simply an extension of AdaGrad that seeks to reduce its\nmonotonically decreasing learning rate. Instead of summing all the\npast gradients, AdaDelta restricts the no. of summation values to a\nlimit (w). In AdaDelta, the sum of past gradients (w) is defined as\n\u201cDecaying Average of all past squared gradients\u201d. The current average\nat the iteration then depends only on the previous average and current\ngradient.\n\nConcept: A refinement of Adagrad that restricts the accumulation of past squared gradients,\n\nallowing learning rates to vary adaptively over time.\nUpdate Rule:\n\nn\u00b09\nVElg\u2019| +e\n\nwith updates scaled by a running average of parameter updates.\n\ne Advantage: No manual tuning of learning rates and solves Adagrad's decay issue.\n6. RMSprop (Root Mean Square) Optimizer\n\n* Root Mean Squared Prop is another adaptive learning rate method that\ntries to improve AdaGrad. Instead of taking cumulative sum of\nsquared gradients like in AdaGrad, we take the exponential moving\naverage. The first step in both AdaGrad and RMSProp 1s identical.\nRMSProp simply divides learning rate by an exponentially decaying\naverage.",
  "Page_58": "\u00a2 Concept: A variant of Adagrad that resolves the issue of decaying learning rates by using an\n\nexponentially weighted moving average of past gradients.\nUpdate Rule:\nu]\n\n/ Elg?| +\u20ac 9\n\nwhere E[g\u201d] is the moving average of squared gradients.\n\nAdvantage: Works well for non-stationary objectives (e.g., deep learning).\n\n7. Adaptive Moment Estimation (Adam) Deep Learning Optimizer\n\n* It is a combination of RMSProp and Momentum. This method\ncomputes adaptive learning rate for each parameter. In addition to\nstoring the previous decaying average of squared gradients, it also\nholds the average of past gradient similar to Momentum. Thus, Adam\nbehaves like a heavy ball with friction which prefers flat minima in\nerror surface.\n\nW.1=(1\u2014-A) w,\u20149V f,(w,)\n\nMomentum update Nesterov momentum update\n\nlookahead\u201d gradient\n\nstep (bit different thar\nmomentum \u2018onginal)\nstep\nactual step\n\nConcept: Combines the benefits of RMSprop and momentum by using adaptive learning rates\n\ngradient\nstep\n\nand maintaining exponentially moving averages of gradients and their squares.\nUpdate Rule:\n\nm\n\" Vite\n\nwhere 7 and @ are bias-corrected estimates of the mean and variance of gradients.\n\n6=6-\n\ne Advantage: Fast convergence and widely used in deep learning."
}